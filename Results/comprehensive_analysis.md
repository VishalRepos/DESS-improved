# DESS (D2E2S) Project - Comprehensive Analysis

**Date**: January 5, 2026  
**Project**: Aspect-Based Sentiment Analysis with Enhanced Graph Convolutional Networks

---

## üìã Executive Summary

This project focuses on **Aspect Sentiment Triplet Extraction (ASTE)** using the DESS (DeBERTa-based Dual Encoder for Entity and Sentiment) model. We've been systematically enhancing the model architecture with advanced Graph Convolutional Network (GCN) techniques to improve triplet extraction performance.

### Current Best Performance:
- **Entity F1**: 88.68%
- **Triplet F1**: 77.14% (+1.39% over baseline)
- **Configuration**: Enhanced Semantic GCN only (without Syntactic GCN)
- **Dataset**: Restaurant 2014 (14res)

---

## üèóÔ∏è Architecture Overview

### Base Model: D2E2S (DeBERTa-based Dual Encoder)

The DESS model consists of several key components:

#### 1. **Transformer Encoder (DeBERTa-v3-base)**
- Pre-trained language model: `microsoft/deberta-v3-base`
- Feature dimension: 768
- Provides contextualized token embeddings
- Replaced original DeBERTa-v2-xxlarge (1536 dim) to avoid OOM errors on P100 GPU

#### 2. **Dual Graph Convolutional Networks**

**A. Syntactic GCN** (Dependency-based)
- Processes syntactic dependency tree structure
- Original: Basic GCN with adjacency matrix
- **Enhanced Version** (Memory-optimized):
  - **GATv2 (Graph Attention Network v2)**: Multi-head attention mechanism for weighted neighbor aggregation
  - **GraphSAGE**: Sample and aggregate approach with mean pooling
  - **Hybrid Fusion**: Attention-based fusion of multiple GCN outputs
  - Reduced from 4 to 2 approaches for memory efficiency

**B. Semantic GCN** (Attention-based)
- Processes semantic relationships between tokens
- Original: Multi-head attention with basic GCN
- **Enhanced Version**:
  - **Relative Position Encoding**: Captures positional relationships (max 128 positions)
  - **Global Context Aggregation**: Weighted pooling across entire sequence
  - **Multi-Scale Feature Extraction**: Combines features from different GCN layers
  - Enhanced multi-head attention with learnable position embeddings

#### 3. **Task-Specific Classifiers**
- **Entity Classifier**: Identifies aspect terms and opinion terms
- **Sentiment Classifier**: Predicts sentiment polarity for aspect-opinion pairs
- Uses concatenated features from DeBERTa + GCNs + size embeddings

#### 4. **Additional Components**
- **BiLSTM**: Bidirectional LSTM for sequential modeling (2 layers, 384 dim)
- **Attention Module**: Self-attention for feature refinement
- **Size Embeddings**: Encodes span size information (100 positions, 25 dim)
- **Dropout & Layer Normalization**: Regularization techniques

---

## üî¨ What We Did So Far

### Phase 1: Infrastructure Setup & Baseline (Dec 30, 2025)

#### 1.1 Model Compatibility Fixes
**Problem**: Original code used DeBERTa-v2-xxlarge (1536 dim) causing OOM on P100 GPU

**Solutions**:
- ‚úÖ Made tokenizer configurable (commit 84c8104)
- ‚úÖ Removed hardcoded model references (commit 9a909f0)
- ‚úÖ Fixed AdamW import from torch.optim (commit f423db5)
- ‚úÖ Removed deprecated `correct_bias` parameter (commit 17fb0ee)

**Impact**: Can now use smaller models like deberta-v3-base (768 dim)

#### 1.2 Baseline Establishment
**Configuration**:
```bash
python train.py --dataset 14res --epochs 120 \
    --pretrained_deberta_name microsoft/deberta-v3-base \
    --deberta_feature_dim 768 --hidden_dim 384 --emb_dim 768
```

**Baseline Results** (Epoch 94):
- Entity F1: 87.65%
- Triplet F1: 75.75%

---

### Phase 2: DeBERTa Enhancement Attempt (Dec 30, 2025)

#### 2.1 Enhanced Dropout & Layer Normalization (commit ef6fbec)
**Hypothesis**: Better regularization would improve generalization

**Changes**:
- Added configurable attention dropout (default: 0.1)
- Added configurable hidden dropout (default: 0.1)
- Implemented layer normalization after DeBERTa output
- Added residual connections: `dropout(layernorm(x)) + x`

**Results**: ‚ùå **No improvement** - Same performance as baseline (75.75%)

**Decision**: Reverted changes (Pending commit) - Enhancement didn't help

---

### Phase 3: Enhanced Semantic GCN (Dec 31, 2025)

#### 3.1 Semantic GCN Enhancements
**Motivation**: Improve semantic relationship modeling between tokens

**Enhancements Implemented**:

1. **Relative Position Encoding**
   - Learnable position embeddings for keys and values
   - Max relative position: 128
   - Captures distance-aware relationships

2. **Global Context Aggregation**
   - Weighted pooling across entire sequence
   - Gating mechanism to control global vs local features
   - Formula: `output = gate * global_context + (1 - gate) * local_features`

3. **Multi-Scale Feature Extraction**
   - Combines features from all GCN layers
   - Learnable scale weights for each layer
   - Adaptive fusion based on importance

**Configuration**:
```bash
python train.py --dataset 14res --epochs 120 \
    --pretrained_deberta_name microsoft/deberta-v3-base \
    --deberta_feature_dim 768 --hidden_dim 384 --emb_dim 768 \
    --use_enhanced_semgcn
```

**Results** (Train 1): ‚≠ê **Best Performance**
- Entity F1: 88.68%
- Triplet F1: 77.14% (+1.39% improvement)
- Best Epoch: 68
- Training stability: Excellent (std dev: 0.26%)

---

### Phase 4: Enhanced Syntactic GCN (Dec 31, 2025)

#### 4.1 Syntactic GCN Enhancements
**Motivation**: Improve dependency tree structure modeling

**Enhancements Implemented**:

1. **GATv2 (Graph Attention Network v2)**
   - Multi-head attention for graph convolution
   - Learnable attention weights for neighbor importance
   - More expressive than standard GAT

2. **GraphSAGE (Sample and Aggregate)**
   - Mean aggregation of neighbor features
   - Concatenates self and neighbor representations
   - Efficient for large graphs

3. **Hybrid Fusion**
   - Attention-based fusion of multiple GCN outputs
   - Learns optimal combination weights
   - Reduces to 2 approaches (from 4) for memory efficiency

**Note**: Originally included Chebyshev and EdgeConv, but removed due to memory constraints

---

### Phase 5: Combined Enhancement Experiment (Dec 31, 2025)

#### 5.1 Testing Both Enhancements Together
**Hypothesis**: Combining both enhanced GCNs would yield best results

**Configuration**:
```bash
python train.py --dataset 14res --epochs 120 \
    --pretrained_deberta_name microsoft/deberta-v3-base \
    --deberta_feature_dim 768 --hidden_dim 384 --emb_dim 768 \
    --use_enhanced_semgcn --use_enhanced_syngcn
```

**Results** (Train 2 & 3): ‚ùå **Worse than SemGCN alone**
- Entity F1: 87.66%
- Triplet F1: 76.99% (+1.24% vs baseline, but -0.15% vs SemGCN only)
- Best Epoch: 101 (slower convergence)

---

## üìä Results Analysis

### Performance Comparison

| Configuration | Entity F1 | Triplet F1 | vs Baseline | Best Epoch |
|--------------|-----------|------------|-------------|------------|
| Baseline | 87.65% | 75.75% | --- | 94 |
| + Enhanced DeBERTa | 87.65% | 75.75% | 0.00% | 94 |
| + Enhanced SemGCN ‚≠ê | 88.68% | **77.14%** | **+1.39%** | 68 |
| + SemGCN + SynGCN | 87.66% | 76.99% | +1.24% | 101 |

### Key Findings

#### ‚úÖ What Worked:
1. **Enhanced Semantic GCN** - Clear winner with +1.39% improvement
   - Relative position encoding helps capture token relationships
   - Global context aggregation provides better sentence-level understanding
   - Multi-scale features leverage information from all layers
   - Faster convergence (epoch 68 vs 94)

2. **Model Size Reduction** - deberta-v3-base works well
   - Avoids OOM errors
   - Maintains competitive performance
   - Faster training

#### ‚ùå What Didn't Work:
1. **Enhanced DeBERTa Dropout** - No improvement
   - Baseline dropout already sufficient
   - Additional regularization didn't help

2. **Combined SemGCN + SynGCN** - Worse than SemGCN alone
   - Model complexity increased optimization difficulty
   - Possible feature interference between semantic and syntactic
   - Memory constraints forced reduction to 2 SynGCN approaches
   - Slower convergence (epoch 101 vs 68)

### Why Combined Approach Failed

**Hypothesis 1: Model Complexity**
- Too many parameters make optimization harder
- More difficult to find optimal weights
- Requires more training data or epochs

**Hypothesis 2: Feature Interference**
- Semantic and syntactic features may conflict
- Different graph structures (attention vs dependency) create inconsistent signals
- Fusion mechanism may not be optimal

**Hypothesis 3: Memory Constraints**
- Had to reduce SynGCN from 4 to 2 approaches
- Lost diversity in syntactic modeling
- Incomplete implementation of original design

**Hypothesis 4: Overfitting**
- More complex model overfits training data
- Doesn't generalize as well to test set
- Needs stronger regularization

**Hypothesis 5: Hyperparameter Mismatch**
- Learning rate optimized for simpler model
- Dropout rates may need adjustment
- Batch size or gradient accumulation may need tuning

---

## üéØ Current Architecture (Best Configuration)

### Model: DESS with Enhanced Semantic GCN

```
Input Text
    ‚Üì
DeBERTa-v3-base (768 dim)
    ‚Üì
Token Embeddings
    ‚Üì
Enhanced Semantic GCN
    ‚îú‚îÄ‚îÄ Multi-head Attention (with relative position)
    ‚îú‚îÄ‚îÄ Global Context Aggregation
    ‚îî‚îÄ‚îÄ Multi-Scale Feature Extraction
    ‚Üì
BiLSTM (2 layers, 384 dim)
    ‚Üì
Self-Attention
    ‚Üì
Task-Specific Classifiers
    ‚îú‚îÄ‚îÄ Entity Classifier ‚Üí Aspect & Opinion Terms
    ‚îî‚îÄ‚îÄ Sentiment Classifier ‚Üí Polarity (Positive/Negative/Neutral)
    ‚Üì
Output: (Aspect, Opinion, Sentiment) Triplets
```

### Key Parameters:
- **Transformer**: microsoft/deberta-v3-base
- **DeBERTa Feature Dim**: 768
- **Hidden Dim**: 384
- **Embedding Dim**: 768
- **LSTM Layers**: 2
- **LSTM Dim**: 384
- **Attention Heads**: 1 (for semantic GCN)
- **GCN Layers**: 2
- **GCN Dropout**: 0.2
- **General Dropout**: 0.5
- **Batch Size**: 16
- **Learning Rate**: 3e-5
- **Epochs**: 120
- **Optimizer**: AdamW

---

## üìà Training Insights

### Convergence Patterns

**Enhanced SemGCN** (Best):
- Fast convergence: Best at epoch 68
- Stable training: Top 10 epochs avg 76.30%, std 0.26%
- Consistent improvement in early epochs

**Combined SemGCN + SynGCN**:
- Slower convergence: Best at epoch 101
- Still stable: Top 10 epochs avg 76.30%, std 0.26%
- Takes longer to find optimal weights

**Baseline**:
- Moderate convergence: Best at epoch 94
- Stable but lower performance

### Dataset: Restaurant 2014 (14res)
- Training samples: 592
- Test samples: 320
- Entity types: Aspect terms (t), Opinion terms (o)
- Sentiment types: Positive, Negative, Neutral

---

## üîÆ Future Directions

### Immediate Next Steps:

1. **Test Enhanced SynGCN Alone** (without SemGCN)
   - Isolate syntactic enhancement impact
   - Compare with SemGCN alone
   - Determine if SynGCN helps independently

2. **Investigate Feature Fusion**
   - Try different fusion strategies (concatenation, gating, attention)
   - Adjust fusion weights between Sem and Syn GCN
   - Add learnable fusion parameters

3. **Hyperparameter Tuning for Combined Model**
   - Lower learning rate (1e-5 or 2e-5)
   - Adjust dropout rates (try 0.3 or 0.4)
   - Increase batch size or use gradient accumulation
   - Add weight decay or other regularization

4. **Reduce SynGCN Complexity**
   - Use only 1 approach (GATv2 or SAGE)
   - Simplify fusion mechanism
   - Reduce number of attention heads

### Long-term Improvements:

1. **Advanced Architectures**
   - Transformer-based GCN (Graph Transformer)
   - Hierarchical attention mechanisms
   - Cross-attention between semantic and syntactic

2. **Training Enhancements**
   - Curriculum learning (easy to hard examples)
   - Multi-task learning with auxiliary tasks
   - Contrastive learning for better representations

3. **Data Augmentation**
   - Back-translation
   - Synonym replacement
   - Dependency tree perturbation

4. **Ensemble Methods**
   - Combine multiple model checkpoints
   - Ensemble different architectures
   - Voting or stacking strategies

5. **Test on Other Datasets**
   - 14lap (Laptop 2014)
   - 15res (Restaurant 2015)
   - 16res (Restaurant 2016)
   - Cross-domain evaluation

---

## üí° Lessons Learned

1. **Simpler is Often Better**
   - Enhanced SemGCN alone outperforms complex combinations
   - Adding more components doesn't guarantee improvement
   - Focus on quality over quantity of enhancements

2. **Memory Constraints Matter**
   - P100 GPU limitations forced design compromises
   - Had to reduce SynGCN approaches from 4 to 2
   - Model size vs performance tradeoff is critical

3. **Feature Interaction is Complex**
   - Semantic and syntactic features may interfere
   - Need careful design of fusion mechanisms
   - Independent testing before combination is important

4. **Baseline Matters**
   - Original model already well-designed
   - Hard to improve significantly without major changes
   - Small improvements (1-2%) are meaningful in this task

5. **Training Stability is Key**
   - Consistent performance across epochs indicates good design
   - Fast convergence suggests effective architecture
   - Monitoring multiple metrics (Entity F1, Triplet F1) is essential

---

## üìù Technical Details

### Enhanced Semantic GCN Components

#### 1. Relative Position Encoding
```python
# Learnable embeddings for relative positions
self.relative_position_k = nn.Embedding(2 * max_relative_position + 1, d_k)
self.relative_position_v = nn.Embedding(2 * max_relative_position + 1, d_k)

# Compute relative positions
relative_positions = self._get_relative_positions(seq_len)
# Clamp to [-128, 128] range
```

#### 2. Global Context Aggregation
```python
# Weighted pooling
global_context = (features * mask).sum(1) / mask.sum(1)
# Gating mechanism
gate = sigmoid(fc(global_context))
output = gate * global_context + (1 - gate) * local_features
```

#### 3. Multi-Scale Feature Extraction
```python
# Collect features from all GCN layers
multi_scale_features = [layer1_out, layer2_out, ...]
# Learnable weights for each scale
scale_weights = softmax(learnable_weights)
# Weighted combination
output = sum(w * f for w, f in zip(scale_weights, features))
```

### Enhanced Syntactic GCN Components

#### 1. GATv2 Layer
```python
# Multi-head attention for graph
h_i = h.unsqueeze(2).expand(...)  # Source nodes
h_j = h.unsqueeze(1).expand(...)  # Target nodes
concat = cat([h_i, h_j])
attention_scores = leakyrelu(sum(concat * learnable_a))
attention_weights = softmax(attention_scores)
output = attention_weights @ h
```

#### 2. GraphSAGE Layer
```python
# Mean aggregation
neighbor_features = adj_normalized @ node_features
# Concatenate self and neighbors
combined = cat([node_features, neighbor_features])
# Transform
output = relu(W @ combined)
```

---

## üéâ Conclusion

We've successfully improved the DESS model by **+1.39%** in Triplet F1 score through **Enhanced Semantic GCN** with relative position encoding, global context aggregation, and multi-scale features. The key insight is that semantic relationships are more important than syntactic dependencies for this task, and simpler focused enhancements outperform complex combinations.

**Best Configuration**: Enhanced Semantic GCN only
- **Entity F1**: 88.68%
- **Triplet F1**: 77.14%
- **Improvement**: +1.39% over baseline
- **Status**: Production-ready ‚úÖ

The project demonstrates the importance of systematic experimentation, careful analysis of results, and understanding when to stop adding complexity. Future work should focus on testing individual components, optimizing hyperparameters, and exploring alternative fusion strategies before attempting more complex architectures.

---

**Last Updated**: January 5, 2026, 21:48 IST
